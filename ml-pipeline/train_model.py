"""
train_model.py - Final Version with Comprehensive Evaluation

T·∫≠p l·ªánh n√†y t·∫£i d·ªØ li·ªáu, hu·∫•n luy·ªán model, tinh ch·ªânh tham s·ªë,
v√† th·ª±c hi·ªán ƒë√°nh gi√° to√†n di·ªán (Confusion Matrix, ROC, PR Curve).
"""

import pandas as pd
import numpy as np
import os
import glob
import joblib
import warnings
import yaml
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import sys

# Th√™m ƒë∆∞·ªùng d·∫´n hi·ªán t·∫°i v√†o sys.path ƒë·ªÉ import c√°c module c√πng th∆∞ m·ª•c
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import LinearSVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score
from sklearn.preprocessing import StandardScaler

# Import module ƒë√°nh gi√° v√† t·ªëi ∆∞u
try:
    from optimization import WOA_SSA_Hybrid
    from model_evaluation import ModelEvaluator
except ImportError as e:
    print(f"‚ö†Ô∏è C·∫£nh b√°o Import: {e}")
    print("   ƒêang ch·∫°y ch·∫ø ƒë·ªô c∆° b·∫£n (kh√¥ng c√≥ Optimization/Evaluation n√¢ng cao).")
    WOA_SSA_Hybrid = None
    ModelEvaluator = None

# T·∫Øt c√°c c·∫£nh b√°o kh√¥ng quan tr·ªçng
warnings.filterwarnings('ignore')

# <<< S·ª¨A T√äN CLASS CHO ƒê√öNG >>>
class DDoSTrainer:
    def __init__(self, config=None):
        self.config = config or {}
        
        # ƒê·∫∂C TR∆ØNG (FEATURES) QUAN TR·ªåNG
        self.feature_names = [
            'protocol', 'tx_packets', 'rx_packets', 'tx_bytes', 'rx_bytes',
            'delay_sum', 'jitter_sum', 'lost_packets', 'packet_loss_ratio',
            'throughput', 'flow_duration'
        ]
        
        self.models = {
            'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=20, random_state=42, class_weight='balanced'),
            'Decision Tree': DecisionTreeClassifier(max_depth=15, random_state=42, class_weight='balanced'),
            'K-Neighbors': KNeighborsClassifier(n_neighbors=5, weights='distance'),
            'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42),
            'SVM (Linear)': LinearSVC(random_state=42, class_weight='balanced', max_iter=2000, dual=False)
        }
        self.trained_models = {}
        self.scaler = StandardScaler()
        self.feature_importance = {}
        self.best_model = None

    def load_data(self, data_dir):
        """T·∫£i v√† g·ªôp t·∫•t c·∫£ file CSV, l√†m s·∫°ch d·ªØ li·ªáu."""
        print(f"üìä ƒêang t·∫£i T·∫§T C·∫¢ b·ªô d·ªØ li·ªáu t·ª´: {data_dir}")

        # T√¨m file kh·ªõp m·∫´u
        search_pattern = os.path.join(data_dir, "ns3_detailed_results_*.csv")
        csv_files = glob.glob(search_pattern)

        # Fallback n·∫øu kh√¥ng t√¨m th·∫•y file m·∫´u
        if not csv_files:
            fallback_file = os.path.join(data_dir, "ns3_detailed_results.csv")
            if os.path.exists(fallback_file):
                csv_files = [fallback_file]
            else:
                raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y file NS3 n√†o t·∫°i {data_dir}")

        print(f"T√¨m th·∫•y {len(csv_files)} file ƒë·ªÉ g·ªôp l·∫°i:")
        
        all_dataframes = []
        for f in csv_files:
            print(f"  - {os.path.basename(f)}")
            try:
                df_temp = pd.read_csv(f, skipinitialspace=True)
                all_dataframes.append(df_temp)
            except Exception as e:
                print(f"  ‚ö†Ô∏è L·ªói khi ƒë·ªçc file {f}: {e}. B·ªè qua.")

        if not all_dataframes:
             raise ValueError("Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c d·ªØ li·ªáu n√†o c·∫£.")

        df = pd.concat(all_dataframes, ignore_index=True)
        print("‚úÖ G·ªôp t·∫•t c·∫£ d·ªØ li·ªáu th√†nh c√¥ng.")
        
        # X√≥a kho·∫£ng tr·∫Øng trong t√™n c·ªôt
        df.columns = df.columns.str.strip()

        if 'label' not in df.columns:
            raise ValueError("Kh√¥ng t√¨m th·∫•y c·ªôt 'label' trong d·ªØ li·ªáu.")
            
        # L·ªçc X v√† y, ƒëi·ªÅn 0 n·∫øu thi·∫øu c·ªôt
        for col in self.feature_names:
            if col not in df.columns:
                # print(f"‚ö†Ô∏è C·∫£nh b√°o: Thi·∫øu c·ªôt '{col}'. ƒêi·ªÅn gi√° tr·ªã 0.")
                df[col] = 0

        X = df[self.feature_names]
        y = df['label']
        
        # X·ª≠ l√Ω d·ªØ li·ªáu b·∫©n
        X = X.replace([np.inf, -np.inf], np.nan)
        X = X.fillna(0)
        
        print(f"T·ªïng k√≠ch th∆∞·ªõc b·ªô d·ªØ li·ªáu g·ªôp: {df.shape}")
        print(f"Ph√¢n b·ªë nh√£n cu·ªëi c√πng:\n{y.value_counts(normalize=True)}")
        
        return X, y
    
    def create_synthetic_data(self, n_samples=10000):
        """T·∫°o d·ªØ li·ªáu gi·∫£ (Fallback)"""
        print("üîÑ Creating synthetic data (matching NS3 columns)...")
        np.random.seed(42)
        
        X = pd.DataFrame(index=range(n_samples), columns=self.feature_names)
        y = np.zeros(n_samples)
        
        n_normal = int(n_samples * 0.8)
        
        # Normal
        X.loc[:n_normal, 'protocol'] = np.random.choice([6, 17], n_normal)
        X.loc[:n_normal, 'tx_packets'] = np.random.normal(50, 10, n_normal)
        X.loc[:n_normal, 'rx_packets'] = np.random.normal(45, 10, n_normal)
        X.loc[:n_normal, 'tx_bytes'] = X['tx_packets'] * 512
        X.loc[:n_normal, 'rx_bytes'] = X['rx_packets'] * 512
        X.loc[:n_normal, 'throughput'] = np.random.normal(100, 20, n_normal)
        X.loc[:n_normal, 'packet_loss_ratio'] = 0.05

        # Attack
        start = n_normal
        X.loc[start:, 'protocol'] = 17
        X.loc[start:, 'tx_packets'] = np.random.normal(5000, 500, n_samples - start)
        X.loc[start:, 'rx_packets'] = np.random.normal(10, 5, n_samples - start)
        X.loc[start:, 'tx_bytes'] = X['tx_packets'] * 1024
        X.loc[start:, 'rx_bytes'] = X['rx_packets'] * 1024
        X.loc[start:, 'throughput'] = np.random.normal(5000, 500, n_samples - start)
        X.loc[start:, 'packet_loss_ratio'] = 0.95
        
        y[start:] = 1
        X = X.fillna(0)
        
        return X, pd.Series(y, name='label')

    def prepare_data(self, X, y):
        """Chia v√† Chu·∫©n h√≥a d·ªØ li·ªáu"""
        print("\nüîÑ Chu·∫©n b·ªã d·ªØ li·ªáu (Split & Scale)...")
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        self.scaler.fit(X_train)
        X_train_scaled = pd.DataFrame(self.scaler.transform(X_train), columns=self.feature_names)
        X_test_scaled = pd.DataFrame(self.scaler.transform(X_test), columns=self.feature_names)
        
        return X_train_scaled, X_test_scaled, y_train, y_test

    def train_baseline(self, X_train, X_test, y_train, y_test):
        """Hu·∫•n luy·ªán model c∆° b·∫£n"""
        print("\n1Ô∏è‚É£ Training Baseline Model (Default Random Forest)...")
        model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
        model.fit(X_train, y_train)
        
        y_pred = model.predict(X_test)
        acc = accuracy_score(y_test, y_pred)
        print(f"   Baseline Accuracy: {acc:.4f}")
        return model, acc

    def run_optimization(self, X_train, y_train, X_test, y_test):
        """Ch·∫°y WOA-SSA ƒë·ªÉ t·ªëi ∆∞u h√≥a"""
        if WOA_SSA_Hybrid is None:
            print("‚ö†Ô∏è Module optimization kh√¥ng t·ªìn t·∫°i. B·ªè qua.")
            return None, 0.0, None

        print("\n2Ô∏è‚É£ Running WOA-SSA Hybrid Optimization...")
        print("   (Qu√° tr√¨nh n√†y c√≥ th·ªÉ m·∫•t v√†i ph√∫t...)")
        
        optimizer = WOA_SSA_Hybrid(population_size=10, max_iter=10) # Gi·∫£m xu·ªëng ƒë·ªÉ ch·∫°y nhanh demo
        best_solution, best_fitness = optimizer.optimize(X_train, y_train)
        
        optimizer.plot_convergence()
        best_model, feature_mask = optimizer.get_optimized_model(X_train, y_train)
        
        # ƒê√°nh gi√° tr√™n t·∫≠p test
        X_test_opt = X_test.iloc[:, feature_mask]
        y_pred = best_model.predict(X_test_opt)
        acc = accuracy_score(y_test, y_pred)
        
        print(f"   ‚ú® Optimized Accuracy: {acc:.4f}")
        return best_model, acc, feature_mask

    def evaluate_and_save(self, model, X_test, y_test, feature_mask, save_path):
        """ƒê√°nh gi√° chi ti·∫øt v√† l∆∞u model"""
        print("\n3Ô∏è‚É£ Final Evaluation & Saving...")
        
        # L·ªçc feature
        if feature_mask is not None:
            X_test_eval = X_test.iloc[:, feature_mask]
            selected_names = np.array(self.feature_names)[feature_mask]
        else:
            X_test_eval = X_test
            selected_names = self.feature_names

        # ƒê√°nh gi√°
        if ModelEvaluator:
            evaluator = ModelEvaluator(model, X_test_eval, y_test, feature_names=list(selected_names))
            evaluator.comprehensive_evaluation()
        else:
            print("‚ö†Ô∏è Module ModelEvaluator kh√¥ng t·ªìn t·∫°i. B·ªè qua v·∫Ω bi·ªÉu ƒë·ªì.")
            print(classification_report(y_test, model.predict(X_test_eval)))

        # L∆∞u model
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        joblib.dump({
            'model': model,
            'scaler': self.scaler,
            'feature_names': list(selected_names),
            'all_feature_names': self.feature_names,
            'timestamp': datetime.now().isoformat()
        }, save_path)
        
        print(f"\n‚úÖ Model saved to: {save_path}")
        print(f"   Features Selected: {len(selected_names)}/{len(self.feature_names)}")

if __name__ == "__main__":
    # C·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi
    BASE_DIR = '/home/traphan/ns-3-dev/ddos-project-new'
    DATA_DIR = os.path.join(BASE_DIR, 'data', 'raw')
    MODELS_DIR = os.path.join(BASE_DIR, 'models')
    
    trainer = DDoSTrainer()
    
    try:
        # 1. Load Data
        X, y = trainer.load_data(DATA_DIR)
        
        # 2. Prepare
        X_train, X_test, y_train, y_test = trainer.prepare_data(X, y)
        
        # 3. Train Baseline
        base_model, base_acc = trainer.train_baseline(X_train, X_test, y_train, y_test)
        
        # 4. Optimize
        opt_model, opt_acc, feat_mask = trainer.run_optimization(X_train, y_train, X_test, y_test)
        
        MODEL_PATH = os.path.join(MODELS_DIR, 'ddos_model.pkl')
        
        if opt_model and opt_acc >= base_acc:
            print(f"\nüèÜ WOA-SSA Model chi·∫øn th·∫Øng ({opt_acc:.4f} vs {base_acc:.4f})")
            trainer.evaluate_and_save(opt_model, X_test, y_test, feat_mask, MODEL_PATH)
        else:
            print(f"\n‚ö†Ô∏è Baseline Model t·ªët h∆°n ho·∫∑c t∆∞∆°ng ƒë∆∞∆°ng ({base_acc:.4f}). L∆∞u Baseline.")
            full_mask = np.ones(len(trainer.feature_names), dtype=bool)
            trainer.evaluate_and_save(base_model, X_test, y_test, full_mask, MODEL_PATH)
            
    except Exception as e:
        print(f"\n‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
        
        print("‚ö†Ô∏è Fallback: Training with synthetic data...")
        X, y = trainer.create_synthetic_data()
        # N·∫øu fallback, ch·∫°y quy tr√¨nh ƒë∆°n gi·∫£n
        trainer.scaler.fit(X) # Fit scaler
        model, acc = trainer.train_baseline(X, X, y, y) # Train tr√™n ch√≠nh n√≥ ƒë·ªÉ test
        
        # L∆∞u model gi·∫£
        os.makedirs(MODELS_DIR, exist_ok=True)
        joblib.dump({
            'model': model,
            'scaler': trainer.scaler,
            'feature_names': trainer.feature_names
        }, os.path.join(MODELS_DIR, 'ddos_model.pkl'))
        print("‚úÖ Saved synthetic model.")
    
    print("\n--- Quy tr√¨nh hu·∫•n luy·ªán ho√†n t·∫•t ---")