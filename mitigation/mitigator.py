import pandas as pd
import joblib
import time
import os
import sys
import warnings
from sklearn.preprocessing import StandardScaler

# T·∫Øt c·∫£nh b√°o feature names kh√¥ng kh·ªõp (ƒë·ªÉ log s·∫°ch h∆°n)
warnings.filterwarnings("ignore", category=UserWarning)

# --- C·∫§U H√åNH ---
BASE_DIR = '/home/traphan/ns-3-dev/ddos-project-new'
MODEL_PATH = os.path.join(BASE_DIR, 'models', 'ddos_model.pkl')

LIVE_DATA_DIR = os.path.join(BASE_DIR, 'data', 'live')
LIVE_STATS_FILE = os.path.join(LIVE_DATA_DIR, 'live_flow_stats.csv')
BLACKLIST_FILE = os.path.join(LIVE_DATA_DIR, 'blacklist.txt')

# Danh s√°ch ƒê·∫¶Y ƒê·ª¶ 11 ƒë·∫∑c tr∆∞ng (C·∫ßn ƒë·ªÉ map d·ªØ li·ªáu ban ƒë·∫ßu)
FULL_FEATURE_COLUMNS = [
    'protocol', 'tx_packets', 'rx_packets', 'tx_bytes', 'rx_bytes',
    'delay_sum', 'jitter_sum', 'lost_packets', 'packet_loss_ratio',
    'throughput', 'flow_duration'
]
# --------------------

class RealTimeMitigator:
    def __init__(self, model_path, stats_file, blacklist_file):
        print("--- Kh·ªüi t·∫°o h·ªá th·ªëng gi·∫£m thi·ªÉu (Mitigation System) ---")
        self.stats_file = stats_file
        self.blacklist_file = blacklist_file
        
        os.makedirs(os.path.dirname(self.stats_file), exist_ok=True)

        # Load Model, Scaler v√† DANH S√ÅCH FEATURE QUAN TR·ªåNG
        self.model, self.scaler, self.selected_features = self._load_model(model_path)
        
        self.last_known_flows = set()
        self.blocked_ips = set()

        if os.path.exists(self.blacklist_file):
            try:
                os.remove(self.blacklist_file)
                print(f"ƒê√£ x√≥a blacklist c≈©: {self.blacklist_file}")
            except OSError:
                pass

        print(f"‚úÖ Model ƒë√£ t·∫£i. M√¥ h√¨nh s·ª≠ d·ª•ng {len(self.selected_features)} ƒë·∫∑c tr∆∞ng: {self.selected_features}")
        print("‚úÖ H·ªá th·ªëng ƒë√£ s·∫µn s√†ng. Ch·ªù d·ªØ li·ªáu t·ª´ NS-3...")

    def _load_model(self, path):
        try:
            if not os.path.exists(path):
                print(f"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file model t·∫°i {path}")
                sys.exit(1)
                
            data = joblib.load(path)
            model = data['model']
            scaler = data['scaler']
            
            # L·∫•y danh s√°ch feature m√† model ƒë√£ h·ªçc (ƒê∆∞·ª£c l∆∞u l√∫c train)
            # N·∫øu kh√¥ng c√≥ key n√†y (model c≈©), m·∫∑c ƒë·ªãnh d√πng full
            selected_features = data.get('feature_names', FULL_FEATURE_COLUMNS)
            
            return model, scaler, selected_features
        except Exception as e:
            print(f"‚ùå L·ªói t·∫£i model: {e}")
            sys.exit(1)

    def _normalize_dataframe(self, df):
        """Chu·∫©n h√≥a t√™n c·ªôt v√† d·ªØ li·ªáu"""
        df.columns = df.columns.str.strip()

        rename_map = {
            'src_ip': 'source_ip',
            'sourceAddress': 'source_ip',
        }
        df = df.rename(columns=rename_map)

        # ƒêi·ªÅn 0 v√†o c√°c c·ªôt thi·∫øu
        for col in FULL_FEATURE_COLUMNS:
            if col not in df.columns:
                df[col] = 0

        df = df.fillna(0)
        return df

    def _process_new_flows(self, new_flows_df):
        if new_flows_df.empty:
            return

        try:
            # B∆Ø·ªöC 1: L·∫•y ƒë·ªß 11 c·ªôt ƒë·ªÉ ƒë∆∞a v√†o Scaler (V√¨ Scaler ƒë∆∞·ª£c fit tr√™n 11 c·ªôt)
            X_full = new_flows_df[FULL_FEATURE_COLUMNS]
            
            # B∆Ø·ªöC 2: Chu·∫©n h√≥a d·ªØ li·ªáu (Scaling)
            # K·∫øt qu·∫£ tr·∫£ v·ªÅ l√† numpy array (m·∫•t t√™n c·ªôt)
            if self.scaler:
                X_scaled_array = self.scaler.transform(X_full)
            else:
                X_scaled_array = X_full.values

            # B∆Ø·ªöC 3: Chuy·ªÉn l·∫°i th√†nh DataFrame ƒë·ªÉ c√≥ t√™n c·ªôt
            X_scaled_df = pd.DataFrame(X_scaled_array, columns=FULL_FEATURE_COLUMNS)
            
            # B∆Ø·ªöC 4: L·ªåC C·ªòT - Ch·ªâ l·∫•y ƒë√∫ng nh·ªØng c·ªôt m√† Model c·∫ßn (3 c·ªôt)
            # ƒê√¢y l√† b∆∞·ªõc quan tr·ªçng ƒë·ªÉ s·ª≠a l·ªói mismatch
            X_final = X_scaled_df[self.selected_features]

            # B∆Ø·ªöC 5: D·ª± ƒëo√°n
            predictions = self.model.predict(X_final)
            new_flows_df['prediction'] = predictions

            # L·ªçc ra c√°c flow t·∫•n c√¥ng (Label = 1)
            attack_flows = new_flows_df[new_flows_df['prediction'] == 1]

            if attack_flows.empty:
                return

            # Ghi v√†o Blacklist
            with open(self.blacklist_file, 'a') as f:
                for ip in attack_flows['source_ip'].unique():
                    if ip not in self.blocked_ips:
                        print(f"üö® PH√ÅT HI·ªÜN T·∫§N C√îNG t·ª´ IP: {ip} -> ƒêang ch·∫∑n...")
                        f.write(f"{ip}\n")
                        f.flush()
                        self.blocked_ips.add(ip)
                        
        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói khi d·ª± ƒëo√°n: {e}")
            # In chi ti·∫øt l·ªói ƒë·ªÉ debug n·∫øu c·∫ßn
            # import traceback
            # traceback.print_exc()

    def watch(self):
        print(f"DEBUG: ƒêang theo d√µi file: {self.stats_file}")

        while not os.path.exists(self.stats_file):
            print(f"DEBUG: ƒêang ch·ªù file {os.path.basename(self.stats_file)} ƒë∆∞·ª£c t·∫°o...")
            time.sleep(1)

        print("DEBUG: File ƒë√£ xu·∫•t hi·ªán. B·∫Øt ƒë·∫ßu ph√¢n t√≠ch...")
        
        last_pos = 0

        while True:
            try:
                # ƒê·ªçc file th√¥ng minh (ch·ªâ ƒë·ªçc ph·∫ßn m·ªõi)
                with open(self.stats_file, 'r') as f:
                    f.seek(last_pos)
                    lines = f.readlines()
                    new_pos = f.tell()
                
                if new_pos == last_pos:
                    time.sleep(0.5)
                    continue
                
                # File b·ªã reset (khi ch·∫°y l·∫°i m√¥ ph·ªèng m·ªõi)
                if new_pos < last_pos:
                    last_pos = 0
                    continue
                
                last_pos = new_pos
                
                if lines:
                    # L·ªçc b·ªè header n·∫øu n√≥ xu·∫•t hi·ªán l·∫°i gi·ªØa file
                    valid_lines = [line for line in lines if "time,source_ip" not in line]
                    if not valid_lines:
                        continue

                    from io import StringIO
                    csv_data = "".join(valid_lines)
                    
                    # Header chu·∫©n kh·ªõp v·ªõi file C++
                    header_names = ["time","source_ip","protocol","tx_packets","rx_packets","tx_bytes","rx_bytes","delay_sum","jitter_sum","lost_packets","packet_loss_ratio","throughput","flow_duration","label"]
                    
                    df = pd.read_csv(StringIO(csv_data), names=header_names, on_bad_lines='skip')
                    
                    df = self._normalize_dataframe(df)
                    
                    # T·∫°o ID duy nh·∫•t: Th·ªùi gian + IP
                    df['record_id'] = df['time'].astype(str) + "-" + df['source_ip'].astype(str)
                    
                    # L·ªçc b·∫£n ghi m·ªõi
                    new_flows_df = df[~df['record_id'].isin(self.last_known_flows)].copy()

                    if not new_flows_df.empty:
                        # print(f"DEBUG: Nh·∫≠n {len(new_flows_df)} d√≤ng d·ªØ li·ªáu m·ªõi.")
                        self._process_new_flows(new_flows_df)
                        
                        # Update cache
                        self.last_known_flows.update(new_flows_df['record_id'])
                        
                        # D·ªçn d·∫πp cache n·∫øu qu√° l·ªõn
                        if len(self.last_known_flows) > 50000:
                            self.last_known_flows.clear()

            except Exception as e:
                print(f"‚ùå L·ªói v√≤ng l·∫∑p ch√≠nh: {e}")
                time.sleep(1)

if __name__ == "__main__":
    mitigator = RealTimeMitigator(
        model_path=MODEL_PATH,
        stats_file=LIVE_STATS_FILE,
        blacklist_file=BLACKLIST_FILE
    )
    mitigator.watch()