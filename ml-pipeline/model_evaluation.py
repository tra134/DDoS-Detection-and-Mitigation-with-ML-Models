import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
from sklearn.metrics import precision_recall_curve, roc_curve, auc, classification_report, confusion_matrix

class ModelEvaluator:
    def __init__(self, model, X_test, y_test, feature_names=None):
        """
        Kh·ªüi t·∫°o b·ªô ƒë√°nh gi√°.
        :param feature_names: Danh s√°ch t√™n c√°c ƒë·∫∑c tr∆∞ng (ƒë·ªÉ v·∫Ω bi·ªÉu ƒë·ªì Feature Importance)
        """
        self.model = model
        self.X_test = X_test
        self.y_test = y_test
        self.feature_names = feature_names
        self.save_dir = '../results'
        
        # T·ª± ƒë·ªông t·∫°o th∆∞ m·ª•c l∆∞u k·∫øt qu·∫£
        os.makedirs(self.save_dir, exist_ok=True)

        # 1. D·ª± ƒëo√°n nh√£n (Label)
        self.y_pred = model.predict(X_test)

        # 2. D·ª± ƒëo√°n x√°c su·∫•t (Probability) - X·ª≠ l√Ω linh ho·∫°t cho SVM v√† RF
        if hasattr(model, "predict_proba"):
            # Random Forest, Decision Tree, etc.
            self.y_pred_proba = model.predict_proba(X_test)[:, 1]
        elif hasattr(model, "decision_function"):
            # SVM (LinearSVC), Gradient Boosting (ƒë√¥i khi)
            # ROC curve c√≥ th·ªÉ l√†m vi·ªác v·ªõi decision_function score
            self.y_pred_proba = model.decision_function(X_test)
        else:
            self.y_pred_proba = None # Kh√¥ng th·ªÉ v·∫Ω ROC/PR
            
    def comprehensive_evaluation(self):
        """ƒê√°nh gi√° to√†n di·ªán model"""
        print("\nüìä Comprehensive Model Evaluation")
        print("=" * 50)
        
        # 1. Classification Report
        print("Classification Report:")
        print(classification_report(self.y_test, self.y_pred))
        
        # 2. Confusion Matrix
        cm = confusion_matrix(self.y_test, self.y_pred)
        self.plot_confusion_matrix(cm)
        
        # 3. ROC & PR Curves (Ch·ªâ v·∫Ω n·∫øu c√≥ x√°c su·∫•t/ƒëi·ªÉm s·ªë)
        if self.y_pred_proba is not None:
            self.plot_roc_curve()
            self.plot_precision_recall_curve()
        
        # 4. Feature Importance (H·ªó tr·ª£ c·∫£ Tree v√† Linear models)
        self.plot_feature_importance()

    def plot_confusion_matrix(self, cm):
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
        plt.title(f'Confusion Matrix - {type(self.model).__name__}')
        plt.ylabel('Actual')
        plt.xlabel('Predicted')
        plt.tight_layout()
        plt.savefig(os.path.join(self.save_dir, 'confusion_matrix.png'), dpi=300)
        plt.show()
        plt.close()

    def plot_roc_curve(self):
        fpr, tpr, _ = roc_curve(self.y_test, self.y_pred_proba)
        roc_auc = auc(fpr, tpr)
        
        plt.figure(figsize=(8, 6))
        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title(f'ROC Curve - {type(self.model).__name__}')
        plt.legend(loc="lower right")
        plt.tight_layout()
        plt.savefig(os.path.join(self.save_dir, 'roc_curve.png'), dpi=300)
        plt.show()
        plt.close()

    def plot_precision_recall_curve(self):
        precision, recall, _ = precision_recall_curve(self.y_test, self.y_pred_proba)
        
        plt.figure(figsize=(8, 6))
        plt.plot(recall, precision, color='blue', lw=2)
        plt.xlabel('Recall')
        plt.ylabel('Precision')
        plt.title(f'Precision-Recall Curve - {type(self.model).__name__}')
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.tight_layout()
        plt.savefig(os.path.join(self.save_dir, 'precision_recall_curve.png'), dpi=300)
        plt.show()
        plt.close()

    def plot_feature_importance(self):
        """V·∫Ω Feature Importance (H·ªó tr·ª£ c·∫£ Random Forest v√† SVM)"""
        importances = None
        
        # Tr∆∞·ªùng h·ª£p 1: C√°c m√¥ h√¨nh c√¢y (Random Forest, Decision Tree)
        if hasattr(self.model, 'feature_importances_'):
            importances = self.model.feature_importances_
            
        # Tr∆∞·ªùng h·ª£p 2: C√°c m√¥ h√¨nh tuy·∫øn t√≠nh (LinearSVC, Logistic Regression)
        elif hasattr(self.model, 'coef_'):
            # L·∫•y gi√° tr·ªã tuy·ªát ƒë·ªëi c·ªßa h·ªá s·ªë ƒë·ªÉ ƒë√°nh gi√° m·ª©c ƒë·ªô ·∫£nh h∆∞·ªüng
            importances = np.abs(self.model.coef_[0])
            
        if importances is None:
            print("‚ÑπÔ∏è Model n√†y kh√¥ng h·ªó tr·ª£ Feature Importance.")
            return

        # N·∫øu kh√¥ng c√≥ t√™n ƒë·∫∑c tr∆∞ng, t·∫°o t√™n gi·∫£ (Feature 0, Feature 1...)
        if self.feature_names is None:
            self.feature_names = [f"Feature {i}" for i in range(len(importances))]
            
        # S·∫Øp x·∫øp v√† v·∫Ω
        indices = np.argsort(importances)[::-1]
        
        plt.figure(figsize=(10, 6))
        plt.title(f"Feature Importance - {type(self.model).__name__}")
        
        # Ch·ªâ v·∫Ω top 15 ƒë·∫∑c tr∆∞ng quan tr·ªçng nh·∫•t ƒë·ªÉ bi·ªÉu ƒë·ªì kh√¥ng b·ªã r·ªëi
        top_n = 15
        plt.bar(range(min(top_n, len(importances))), importances[indices][:top_n], align="center")
        plt.xticks(range(min(top_n, len(importances))), 
                   [self.feature_names[i] for i in indices[:top_n]], rotation=45, ha='right')
        plt.tight_layout()
        plt.savefig(os.path.join(self.save_dir, 'feature_importance.png'), dpi=300)
        plt.show()
        plt.close()