"""
train_model.py

T·∫≠p l·ªánh n√†y t·∫£i t·∫•t c·∫£ d·ªØ li·ªáu m√¥ ph·ªèng NS3 t·ª´ th∆∞ m·ª•c data/raw,
g·ªôp ch√∫ng l·∫°i, sau ƒë√≥ hu·∫•n luy·ªán nhi·ªÅu m√¥ h√¨nh ML ƒë·ªÉ t√¨m ra
m√¥ h√¨nh ph√°t hi·ªán DDoS t·ªët nh·∫•t v√† l∆∞u n√≥ l·∫°i.
"""

import pandas as pd
import numpy as np
import os
import glob  # ƒê·ªÉ t√¨m ki·∫øm file
import joblib
import warnings
import json
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns
import yaml

from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import LinearSVC  # Nhanh h∆°n SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score
from sklearn.preprocessing import StandardScaler

# T·∫Øt c√°c c·∫£nh b√°o kh√¥ng quan tr·ªçng
warnings.filterwarnings('ignore')


class DDoSTrainer:
    def __init__(self, config=None):
        self.config = config or {}
        
        # S·ª¨A: ƒê·ªãnh nghƒ©a r√µ r√†ng c√°c ƒë·∫∑c tr∆∞ng CH·ªà L√Ä S·ªê
        # ƒê√¢y l√† c√°c c·ªôt duy nh·∫•t ch√∫ng ta d√πng ƒë·ªÉ hu·∫•n luy·ªán.
        self.feature_names = [
            'protocol', 'tx_packets', 'rx_packets', 'tx_bytes', 'rx_bytes',
            'delay_sum', 'jitter_sum', 'lost_packets', 'packet_loss_ratio',
            'throughput', 'flow_duration'
        ]
        
        self.models = {
            'Random Forest': RandomForestClassifier(
                n_estimators=100, 
                max_depth=20, 
                random_state=42,
                class_weight='balanced'
            ),
            'Decision Tree': DecisionTreeClassifier(
                max_depth=15, 
                random_state=42,
                class_weight='balanced'
            ),
            'K-Neighbors': KNeighborsClassifier(n_neighbors=5, weights='distance'),
            'Gradient Boosting': GradientBoostingClassifier(
                n_estimators=100, 
                learning_rate=0.1, 
                random_state=42
            ),
            'SVM (Linear)': LinearSVC(random_state=42, class_weight='balanced', max_iter=2000, dual=True)
        }
        self.trained_models = {}
        self.scaler = StandardScaler()
        self.feature_importance = {}
        self.best_model = None

    def load_data(self, data_dir):
        """
        S·ª¨A: T·∫£i v√† g·ªôp t·∫•t c·∫£ file CSV, sau ƒë√≥ CH·ªà CH·ªåN
        c√°c ƒë·∫∑c tr∆∞ng (feature_names) ƒë√£ ƒë·ªãnh nghƒ©a.
        """
        print(f"üìä ƒêang t·∫£i T·∫§T C·∫¢ b·ªô d·ªØ li·ªáu t·ª´: {data_dir}")

        search_pattern = os.path.join(data_dir, "ns3_detailed_results_*_nodes.csv")
        csv_files = glob.glob(search_pattern)

        if not csv_files:
            raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y file NS3 n√†o t·∫°i {search_pattern}")

        print(f"T√¨m th·∫•y {len(csv_files)} file ƒë·ªÉ g·ªôp l·∫°i:")
        all_dataframes = [pd.read_csv(file_path) for file_path in csv_files]
        df = pd.concat(all_dataframes, ignore_index=True)
        print("‚úÖ G·ªôp t·∫•t c·∫£ d·ªØ li·ªáu th√†nh c√¥ng.")
        
        # S·ª¨A: L·ªçc X ƒë·ªÉ CH·ªà ch·ª©a c√°c ƒë·∫∑c tr∆∞ng (features) ƒë√£ ƒë·ªãnh nghƒ©a
        if 'label' not in df.columns:
            raise ValueError("Kh√¥ng t√¨m th·∫•y c·ªôt 'label' trong d·ªØ li·ªáu.")
            
        try:
            # X (Features) ch·ªâ bao g·ªìm c√°c c·ªôt trong self.feature_names
            X = df[self.feature_names] 
            y = df['label']
        except KeyError as e:
            print(f"L·ªói: Kh√¥ng t√¨m th·∫•y c√°c ƒë·∫∑c tr∆∞ng c·∫ßn thi·∫øt trong file CSV. Thi·∫øu: {e}")
            raise
        
        # X·ª≠ l√Ω c√°c gi√° tr·ªã kh√¥ng h·ª£p l·ªá (v√≠ d·ª•: inf)
        X = X.replace([np.inf, -np.inf], np.nan)
        X = X.fillna(0)
        
        print(f"T·ªïng k√≠ch th∆∞·ªõc b·ªô d·ªØ li·ªáu g·ªôp: {df.shape}")
        print(f"S·ªë l∆∞·ª£ng ƒë·∫∑c tr∆∞ng ƒë√£ ch·ªçn: {X.shape[1]}")
        print(f"Ph√¢n b·ªë nh√£n cu·ªëi c√πng:\n{y.value_counts(normalize=True)}")
        
        return X, y
    
    def create_synthetic_data(self, n_samples=10000):
        """
        S·ª¨A: T·∫°o d·ªØ li·ªáu gi·∫£ kh·ªõp v·ªõi c√°c ƒë·∫∑c tr∆∞ng c·ªßa NS3.
        """
        print("üîÑ Creating synthetic data (matching NS3 columns)...")
        np.random.seed(42)
        
        X = pd.DataFrame(index=range(n_samples), columns=self.feature_names)
        y = np.zeros(n_samples)
        
        # T·∫°o d·ªØ li·ªáu normal (80%)
        n_normal = int(n_samples * 0.8)
        X.loc[:n_normal, 'protocol'] = np.random.choice([6, 17], n_normal) # TCP/UDP
        X.loc[:n_normal, 'tx_packets'] = np.random.normal(50, 10, n_normal)
        X.loc[:n_normal, 'rx_packets'] = np.random.normal(45, 10, n_normal)
        X.loc[:n_normal, 'tx_bytes'] = X['tx_packets'] * 512
        X.loc[:n_normal, 'rx_bytes'] = X['rx_packets'] * 512
        X.loc[:n_normal, 'delay_sum'] = np.random.normal(0.5, 0.1, n_normal)
        X.loc[:n_normal, 'jitter_sum'] = np.random.normal(0.1, 0.05, n_normal)
        X.loc[:n_normal, 'lost_packets'] = np.random.randint(0, 5, n_normal)
        X.loc[:n_normal, 'packet_loss_ratio'] = X['lost_packets'] / (X['tx_packets'] + 1)
        X.loc[:n_normal, 'flow_duration'] = np.random.normal(10, 2, n_normal)
        X.loc[:n_normal, 'throughput'] = (X['rx_bytes'] * 8) / (X['flow_duration'] * 1000 + 1) # Kbps

        # T·∫°o d·ªØ li·ªáu attack (20%)
        n_attack = n_samples - n_normal
        start_index = n_normal
        X.loc[start_index:, 'protocol'] = 17 # UDP
        X.loc[start_index:, 'tx_packets'] = np.random.normal(5000, 500, n_attack)
        X.loc[start_index:, 'rx_packets'] = np.random.normal(10, 5, n_attack) # B·ªã server drop
        X.loc[start_index:, 'tx_bytes'] = X['tx_packets'] * 1024
        X.loc[start_index:, 'rx_bytes'] = X['rx_packets'] * 1024
        X.loc[start_index:, 'delay_sum'] = np.random.normal(2.0, 0.5, n_attack) # Delay cao
        X.loc[start_index:, 'jitter_sum'] = np.random.normal(1.0, 0.2, n_attack)
        X.loc[start_index:, 'lost_packets'] = np.random.normal(4900, 500, n_attack)
        X.loc[start_index:, 'packet_loss_ratio'] = X['lost_packets'] / (X['tx_packets'] + 1)
        X.loc[start_index:, 'flow_duration'] = np.random.normal(5, 1, n_attack) # Ng·∫Øn
        X.loc[start_index:, 'throughput'] = (X['rx_bytes'] * 8) / (X['flow_duration'] * 1000 + 1)
        
        y[start_index:] = 1  # Attack labels
        
        X = X.fillna(0)
        y_series = pd.Series(y, name='label')
        
        print(f"Synthetic dataset created: {X.shape}")
        return X, y_series
    
    def train_models(self, X, y):
        """
        S·ª¨A: X_train ƒë√£ l√† DataFrame ch·ªâ ch·ª©a c√°c c·ªôt s·ªë.
        Kh√¥ng c·∫ßn X_train[self.feature_names] n·ªØa.
        """
        # 1. Chia d·ªØ li·ªáu
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        # 2. S·ª¨A: Scaling (Fit tr√™n X_train, transform tr√™n c·∫£ hai)
        print("\nüîÑ Scaling data (Fit on train, transform train/test)...")
        self.scaler.fit(X_train) # X_train ƒë√£ l√† DataFrame ch·ªâ ch·ª©a c√°c ƒë·∫∑c tr∆∞ng s·ªë
        X_train_scaled = self.scaler.transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        print("\nüéØ Training models...")
        results = {}
        
        for name, model in self.models.items():
            print(f"\nüìà Training {name}...")
            model.fit(X_train_scaled, y_train) 
            self.trained_models[name] = model
            
            y_pred = model.predict(X_test_scaled)
            
            # T√≠nh to√°n AUC
            try:
                if hasattr(model, "predict_proba"):
                    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]
                else: 
                    decision_values = model.decision_function(X_test_scaled)
                    y_pred_proba = (decision_values - decision_values.min()) / (decision_values.max() - decision_values.min())
            except Exception:
                y_pred_proba = y_pred
            
            accuracy = accuracy_score(y_test, y_pred)
            if len(np.unique(y_test)) > 1:
                auc_score = roc_auc_score(y_test, y_pred_proba)
            else:
                auc_score = 0.0
                
            results[name] = {'accuracy': accuracy, 'auc_score': auc_score, 'model': model}
            
            print(f"‚úÖ {name} Results: Accuracy: {accuracy:.4f}, AUC Score: {auc_score:.4f}")
            
            # Feature importance
            if hasattr(model, 'feature_importances_'):
                self.feature_importance[name] = model.feature_importances_
            elif hasattr(model, 'coef_'):
                self.feature_importance[name] = np.abs(model.coef_[0])
        
        # Ch·ªçn model t·ªët nh·∫•t
        best_model_name = max(results, key=lambda x: results[x]['auc_score'])
        self.best_model = results[best_model_name]['model']
        
        print(f"\nüèÜ Best Model: {best_model_name}")
        
        # Hi·ªÉn th·ªã detailed report
        y_pred_best = self.best_model.predict(X_test_scaled)
        print(f"\nüìä Detailed Report for {best_model_name}:")
        print(classification_report(y_test, y_pred_best))
        print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_best))
        
        return results, self.best_model, X_train_scaled, X_test_scaled, y_train, y_test
    
    def hyperparameter_tuning(self, X_train, y_train):
        """Tinh ch·ªânh tr√™n X_train ƒë√£ ƒë∆∞·ª£c scale"""
        print("\nüîß Performing hyperparameter tuning...")
        
        best_model_name = type(self.best_model).__name__
        param_grid = {}
        model_to_tune = None
        
        if 'RandomForest' in best_model_name:
            param_grid = {'n_estimators': [100, 200], 'max_depth': [10, 20]}
            model_to_tune = RandomForestClassifier(random_state=42, class_weight='balanced')
        elif 'GradientBoosting' in best_model_name:
            param_grid = {'n_estimators': [100, 200], 'learning_rate': [0.1, 0.05]}
            model_to_tune = GradientBoostingClassifier(random_state=42)
        else:
            print(f"No parameter grid defined for {best_model_name}. Skipping tuning.")
            return self.best_model

        grid_search = GridSearchCV(model_to_tune, param_grid, cv=3, scoring='accuracy', n_jobs=-1, verbose=1)
        grid_search.fit(X_train, y_train) # D√πng X_train ƒë√£ scale
        
        print(f"Best parameters: {grid_search.best_params_}")
        self.best_model = grid_search.best_estimator_
        return self.best_model
    
    def plot_feature_importance(self, save_path=None):
        """V·∫Ω bi·ªÉu ƒë·ªì feature importance"""
        if not self.feature_importance:
            print("No feature importance data available")
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        axes = axes.ravel()
        
        plot_count = 0
        for idx, (name, importance) in enumerate(list(self.feature_importance.items())):
            if plot_count >= 4: break
            if importance is None: continue
                
            indices = np.argsort(importance)[::-1][:10] # Top 10
            axes[plot_count].barh(range(len(indices)), importance[indices])
            axes[plot_count].set_yticks(range(len(indices)))
            axes[plot_count].set_yticklabels([self.feature_names[i] for i in indices])
            axes[plot_count].set_title(f'Feature Importance - {name}')
            axes[plot_count].set_xlabel('Importance')
            plot_count += 1
        
        plt.tight_layout()
        if save_path:
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"Feature importance plot saved to {save_path}")
        plt.show()
    
    def save_model(self, model, file_path):
        """S·ª¨A: T·ª± t·∫°o th∆∞ m·ª•c"""
        model_directory = os.path.dirname(file_path)
        os.makedirs(model_directory, exist_ok=True)
        
        joblib.dump({
            'model': model,
            'scaler': self.scaler,
            'feature_names': self.feature_names,
            'training_time': datetime.now().isoformat(),
            'config': self.config
        }, file_path)
        
        print(f"‚úÖ Model saved to {file_path}")
    
    def train_with_sample_data(self, save_path):
        """Train v·ªõi d·ªØ li·ªáu m·∫´u (fallback)"""
        X, y = self.create_synthetic_data(10000)
        results, best_model, _, _, _, _ = self.train_models(X, y)
        self.save_model(best_model, save_path)
        return results, best_model

# --- H√ÄM CH·∫†Y CH√çNH (MAIN) ---
if __name__ == "__main__":
    
    # S·ª¨A: D√πng ƒë∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi
    BASE_DIR = '/home/traphan/ns-3-dev/ddos-project-new'
    
    CONFIG_PATH = os.path.join(BASE_DIR, 'config', 'ml-config.yaml')
    DATA_DIR = os.path.join(BASE_DIR, 'data', 'raw') 
    RESULTS_DIR = os.path.join(BASE_DIR, 'results')
    MODELS_DIR = os.path.join(BASE_DIR, 'models')
    
    MODEL_SAVE_PATH = os.path.join(MODELS_DIR, 'ddos_model.pkl')
    FEATURES_SAVE_PATH = os.path.join(RESULTS_DIR, 'feature_importance.png')
    
    os.makedirs(RESULTS_DIR, exist_ok=True)
    os.makedirs(MODELS_DIR, exist_ok=True)

    try:
        with open(CONFIG_PATH, 'r') as f:
            config = yaml.safe_load(f)
    except Exception:
        config = {}
    
    trainer = DDoSTrainer(config)
    
    try:
        print(f"--- B·∫Øt ƒë·∫ßu quy tr√¨nh hu·∫•n luy·ªán ---")
        X, y = trainer.load_data(DATA_DIR) 
        
        results, best_model, X_train_s, X_test_s, y_train, y_test = trainer.train_models(X, y)
        
        best_model = trainer.hyperparameter_tuning(X_train_s, y_train)
        
        trainer.plot_feature_importance(FEATURES_SAVE_PATH)
        
        trainer.save_model(best_model, MODEL_SAVE_PATH)
        
    except FileNotFoundError:
        print(f"‚ùå L·ªñI: Kh√¥ng t√¨m th·∫•y file d·ªØ li·ªáu NS3 n√†o trong {DATA_DIR}.")
        print("Chuy·ªÉn sang d√πng d·ªØ li·ªáu synthetic (d·ªØ li·ªáu gi·∫£)...")
        trainer.train_with_sample_data(MODEL_SAVE_PATH)
    
    print("\n--- Quy tr√¨nh hu·∫•n luy·ªán ho√†n t·∫•t ---")